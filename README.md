# ğŸ“„ RAG-based PDF Analyzer

A **Retrieval-Augmented Generation (RAG) pipeline** for querying unstructured PDF documents. The system combines **document parsing, embeddings, vector database retrieval, and LLM-based answering**, all wrapped in an easy-to-use **Streamlit UI**.

---

## âœ¨ Features

* ğŸ“‘ **PDF Ingestion** â€“ Extracts and chunks text from uploaded PDF documents.
* ğŸ” **Semantic Search** â€“ Embeds document chunks with **SentenceTransformers** and stores them in **ChromaDB**.
* ğŸ† **Cross-Encoder Reranking** â€“ Improves retrieval precision by reranking top candidates.
* ğŸ¤– **Context-Aware Answers** â€“ Uses LLMs (local or API-based) to generate responses grounded in document context.
* âš¡ **GPU Optimized** â€“ Supports running embeddings and reranking models on GPU (FP16 precision, batch size tuning).
* ğŸ›ï¸ **Config-Driven Architecture** â€“ Easily switch between embedding backends (Hugging Face, Ollama, OpenAI).
* ğŸŒ **Interactive UI** â€“ Upload PDFs and ask natural language questions via **Streamlit**.

---

## ğŸ—ï¸ Architecture

```
PDF Upload â†’ Text Chunking â†’ Embedding (SentenceTransformers) â†’ 
ChromaDB (Vector Store) â†’ Retriever â†’ Reranker (CrossEncoder) â†’ 
LLM (Answer Generator) â†’ Streamlit UI
```

---

## âš™ï¸ Tech Stack

* **Python** (3.9+)
* **PyTorch** â€“ Deep learning backend
* **SentenceTransformers** â€“ Embedding models
* **ChromaDB** â€“ Vector store for semantic retrieval
* **Transformers** â€“ Cross-encoder and reranking models
* **Streamlit** â€“ UI for PDF upload & querying
* **Ollama / OpenAI (Optional)** â€“ LLM integration

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/rag-pdf-analyzer.git
cd rag-pdf-analyzer
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
conda create -n rag_pipeline python=3.9
conda activate rag_pipeline
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure

Update `config.yaml` with your preferences:

```yaml
embedding_model: sentence-transformers/all-MiniLM-L6-v2
cross_encoder: cross-encoder/ms-marco-MiniLM-L-6-v2
llm_backend: openai   # options: openai / ollama / hf
openai_api_key: "your_api_key_here"
```

### 5ï¸âƒ£ Run the App

```bash
streamlit run app.py
```

---

## ğŸ–¼ï¸ Demo

Upload any PDF, ask questions like:

* *â€œWhat is the main conclusion of this report?â€*
* *â€œList all mentioned stakeholders.â€*
* *â€œSummarize section 3 in 2 sentences.â€*

---

## ğŸ“Š Performance Considerations

* Default embedding model: **MiniLM (lightweight, \~22M params)**
* For higher accuracy: try **MPNet** or **bge-large-en** (larger, VRAM-heavy).
* Use **batch\_size=8/16** for GPU memory efficiency.
* Cross-encoder reranking is optional (trade-off: better relevance vs. slower inference).

---

## ğŸ“‚ Project Structure

```
rag-pdf-analyzer/
â”‚â”€â”€ app.py              # Streamlit entry point
â”‚â”€â”€ rag_pipeline.py     # Core RAG pipeline (retriever, reranker, generator)
â”‚â”€â”€ config.yaml         # Config file for models, API keys, parameters
â”‚â”€â”€ requirements.txt    # Python dependencies
â”‚â”€â”€ utils/              # Helpers (PDF parsing, chunking, etc.)
â”‚â”€â”€ docs/               # Architecture diagrams, screenshots
```

---

## ğŸ”® Future Enhancements

* [ ] Multi-PDF support
* [ ] Metadata-aware search (e.g., by section/page)
* [ ] Hybrid retrieval (BM25 + embeddings)
* [ ] Caching frequent queries
* [ ] Dockerized deployment

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!
Feel free to open a PR or raise an issue in the repo.

---

## ğŸ“œ License

This project is licensed under the MIT License.